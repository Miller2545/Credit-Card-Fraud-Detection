{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aed4868-4c84-414f-9372-109d9ef2a521",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596709c-9106-49fc-a8f1-07c33df882d0",
   "metadata": {},
   "source": [
    "# Initial Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155640e-0523-40ae-8b7c-676f629486ca",
   "metadata": {},
   "source": [
    "I found this data on Kaggle at, https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023 . My initial observations are that the amount variable may need to be transformed in the preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe3a77-8fc1-40bf-bd2f-5f104c078704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb0ee27-f764-4a60-97e1-e7f5d4a871a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sea.set(style='ticks')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01065c38-8e21-44cf-bfac-43a7e9bc65fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a139e59e-c4fe-475d-a803-17fcadc394a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "card_data = pd.read_csv(\"C:/Users/james/OneDrive/Desktop/Datasets for Projects/Credit Card Fraud Detection/creditcard_2023.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc9d9f-4134-4767-813d-5ce248ae13a5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c167788-866e-47bb-bde1-3f00fbdc67eb",
   "metadata": {},
   "source": [
    "To preprocess the data, I am scaling all values using the scale method from sklearn preprocessing. From this process I am splitting my x and y variables before variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f5c7ce6-a91f-4396-a34f-c916948181f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_data = card_data.drop(columns = 'id')\n",
    "preprocessed = pd.DataFrame(preprocessing.scale(card_data.drop('Class', axis = 'columns')))\n",
    "preprocessed['Class'] = card_data['Class']\n",
    "preprocessed.columns = card_data.columns\n",
    "x = preprocessed.columns.drop('Class')\n",
    "y = ['Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9d05e-b5b0-45e9-934b-9525c96e6001",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d3aeb-e878-45d3-b018-387c1cd05319",
   "metadata": {},
   "source": [
    "To eliminate features I decided to use the recursive feature elimination to bring down the total amount of variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef7f2b10-8bed-450f-b647-a39f7a414d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True False False  True  True  True  True  True  True\n",
      " False  True False  True  True  True False False False  True False False\n",
      " False False False False False]\n",
      "[ 1  8  1  1 15  3  1  1  1  1  1  1 14  1  5  1  1  1 13 11  4  1  2  6\n",
      "  7 12 10  9 16]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg)\n",
    "rfe = rfe.fit(preprocessed[x], preprocessed[y].values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b90d0-9d37-4780-89f1-6bb219531418",
   "metadata": {},
   "source": [
    "Taking the output, I then created a list of the final variables by using a for loop to prevent having to manually sort through the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4683b45-15a3-45af-bab2-1bcaf7298d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "list = rfe.support_.tolist()\n",
    "final_variables = []\n",
    "for i in list:\n",
    "    if i == True:\n",
    "        final_variables.append(x[index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55301084-3b2b-404c-8d3a-50f75791dae6",
   "metadata": {},
   "source": [
    "Next I am checking the p-values of the chosen variables to ensure I don't need to do another pass of feature elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52f6e4d4-8258-498c-921d-6bcb570cda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.144060\n",
      "         Iterations 9\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Method:           MLE        \n",
      "Dependent Variable: Class            Pseudo R-squared: 0.792      \n",
      "Date:               2023-09-23 13:55 AIC:              163861.5868\n",
      "No. Observations:   568630           BIC:              164019.1006\n",
      "Df Model:           13               Log-Likelihood:   -81917.    \n",
      "Df Residuals:       568616           LL-Null:          -3.9414e+05\n",
      "Converged:          1.0000           LLR p-value:      0.0000     \n",
      "No. Iterations:     9.0000           Scale:            1.0000     \n",
      "--------------------------------------------------------------------\n",
      "          Coef.    Std.Err.       z       P>|z|     [0.025    0.975]\n",
      "--------------------------------------------------------------------\n",
      "V1       -0.3546     0.0106    -33.4496   0.0000   -0.3754   -0.3338\n",
      "V3       -0.3596     0.0116    -30.9643   0.0000   -0.3824   -0.3369\n",
      "V4        2.3810     0.0164    145.3866   0.0000    2.3489    2.4131\n",
      "V7        0.3479     0.0116     30.0981   0.0000    0.3252    0.3705\n",
      "V8       -0.6288     0.0105    -59.9903   0.0000   -0.6494   -0.6083\n",
      "V9        0.2744     0.0127     21.5853   0.0000    0.2495    0.2993\n",
      "V10      -0.2649     0.0133    -19.8679   0.0000   -0.2911   -0.2388\n",
      "V11       0.4471     0.0123     36.4298   0.0000    0.4230    0.4711\n",
      "V12      -1.3017     0.0166    -78.5269   0.0000   -1.3342   -1.2692\n",
      "V14      -1.8364     0.0151   -121.9720   0.0000   -1.8660   -1.8069\n",
      "V16       0.0522     0.0154      3.4006   0.0007    0.0221    0.0823\n",
      "V17       0.7212     0.0167     43.2639   0.0000    0.6885    0.7539\n",
      "V18      -0.0575     0.0156     -3.6870   0.0002   -0.0881   -0.0269\n",
      "V22       0.3227     0.0115     28.0685   0.0000    0.3001    0.3452\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed[final_variables]\n",
    "y = card_data['Class']\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bb3cc-a662-475f-8585-2082ee7fa79a",
   "metadata": {},
   "source": [
    "## Data Splitting and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8189ad-75ea-4b3a-bc90-1b3d9c7458a6",
   "metadata": {},
   "source": [
    "Now to split the data into the testing and training data with a test size of 15% from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e106cbb-f66b-4234-a2b0-72cac3b783a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e35831-f049-4048-8e69-db6fe15bad57",
   "metadata": {},
   "source": [
    "## Model Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d355819-48e6-4e1f-865b-f3b326c017af",
   "metadata": {},
   "source": [
    "Finishing up by creating the predictions from the trained model to move onto assessing performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32f5328e-c97d-4901-bf20-9d66346e1edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, ..., 0, 1, 1], dtype=int64),\n",
       " array([1, 1, 1, ..., 0, 1, 0], dtype=int64))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predictions = logreg.predict(X_train)\n",
    "y_test_predictions = logreg.predict(X_test)\n",
    "y_train_predictions, y_test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fade625-af64-41d1-a0f9-a7838d18ba29",
   "metadata": {},
   "source": [
    "## Assessing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b754578-f8f0-40b9-9496-7e50b5621bd6",
   "metadata": {},
   "source": [
    "Last but not least I created a dataframe to allow for the performance of the model to be easily compared and to allow for comparison of different model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95ab9f62-f3d6-4c0a-b464-1be4a1c7cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Train R^2</th>\n",
       "      <th>Test R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.035094</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>0.859625</td>\n",
       "      <td>0.858749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Train MSE  Test MSE Train R^2  Test R^2\n",
       "0  Logistic Regression  0.035094  0.035313  0.859625  0.858749"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_train_MSE = mean_squared_error(y_train, y_train_predictions)\n",
    "lr_train_R2 = r2_score(y_train, y_train_predictions)\n",
    "\n",
    "lr_test_MSE = mean_squared_error(y_test, y_test_predictions)\n",
    "lr_test_R2 = r2_score(y_test, y_test_predictions)\n",
    "\n",
    "results = pd.DataFrame(['Logistic Regression', lr_train_MSE, lr_test_MSE, lr_train_R2, lr_test_R2]).transpose()\n",
    "results.columns = ['Model', 'Train MSE', 'Test MSE', 'Train R^2', 'Test R^2']\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd5ebb-3832-42e2-a820-f4326d8852ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402ab43-c529-45cf-b26d-1d272c7e4343",
   "metadata": {},
   "source": [
    "Due to this data being anonymized, I wasn't able to fully learn exactly what each variable meant when it contributes to detecting credit card fraud. One thing I was surprised to find out was that when I used the recursive feature elimination method to choose variables, it recommended the removal of the purchase amount. Even after processing the variable to standardize the values it still raised the performance of the model in the end. I have decided to analyze performance of the model using the Mean Square Error and the R^2 value. In the end I added the performance of the model to a pandas dataframe to allow for easier comparison of multiple models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f3fe1-2b64-4a31-81e3-b5e3910b4c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
